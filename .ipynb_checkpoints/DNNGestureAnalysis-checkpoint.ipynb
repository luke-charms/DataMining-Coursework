{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e270444f-3803-48ef-8fb5-22a8c3a50a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "982606bc-72ac-494f-bb67-34d362975b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\Talha\\Documents\\DataScience\\Data Mining\\Project\\DataMining-Coursework\n",
      "Checking DATA_DIR exists: C:\\Users\\Talha\\Documents\\DataScience\\Data Mining\\Project\\DataMining-Coursework\\combined\n",
      "\n",
      "Found 2700 .txt files under: C:\\Users\\Talha\\Documents\\DataScience\\Data Mining\\Project\\DataMining-Coursework\\combined\n",
      "\n",
      " 1. afternoon_apurve_1.txt\n",
      " 2. afternoon_apurve_2.txt\n",
      " 3. afternoon_apurve_3.txt\n",
      " 4. afternoon_apurve_4.txt\n",
      " 5. afternoon_apurve_5.txt\n",
      " 6. afternoon_apurve_6.txt\n",
      " 7. afternoon_apurve_7.txt\n",
      " 8. afternoon_apurve_8.txt\n",
      " 9. afternoon_apurve_9.txt\n",
      "10. afternoon_gautam_1.txt\n",
      "11. afternoon_gautam_2.txt\n",
      "12. afternoon_gautam_3.txt\n",
      "13. afternoon_gautam_4.txt\n",
      "14. afternoon_gautam_5.txt\n",
      "15. afternoon_gautam_6.txt\n",
      "16. afternoon_gautam_7.txt\n",
      "17. afternoon_gautam_8.txt\n",
      "18. afternoon_gautam_9.txt\n",
      "19. afternoon_mahendra_1.txt\n",
      "20. afternoon_mahendra_2.txt\n",
      "21. afternoon_mahendra_3.txt\n",
      "22. afternoon_mahendra_4.txt\n",
      "23. afternoon_mahendra_5.txt\n",
      "24. afternoon_mahendra_6.txt\n",
      "25. afternoon_mahendra_7.txt\n",
      "26. afternoon_mahendra_8.txt\n",
      "27. afternoon_mahendra_9.txt\n",
      "28. afternoon_parveen_1.txt\n",
      "29. afternoon_parveen_2.txt\n",
      "30. afternoon_parveen_3.txt\n",
      "\n",
      "Saved file index to: C:\\Users\\Talha\\Documents\\DataScience\\Data Mining\\Project\\DataMining-Coursework\\combined\\file_index.csv\n"
     ]
    }
   ],
   "source": [
    "# --- USER: set the folder that contains all your .txt files ---\n",
    "DATA_DIR = r\"C:\\Users\\Talha\\Documents\\DataScience\\Data Mining\\Project\\DataMining-Coursework\\combined\"\n",
    "\n",
    "# helpful checks\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Checking DATA_DIR exists:\", DATA_DIR)\n",
    "if not os.path.isdir(DATA_DIR):\n",
    "    raise NotADirectoryError(f\"Path does not exist or is not a directory: {DATA_DIR}\")\n",
    "\n",
    "# find all .txt files (recursive)\n",
    "txt_files = sorted(glob.glob(os.path.join(DATA_DIR, \"**\", \"*.txt\"), recursive=True))\n",
    "\n",
    "#os.path.join(DATA_DIR, \"**\", \"*.txt\")\n",
    "#os.path.join safely builds a file path that works on all operating systems (Windows, Linux, Mac).\n",
    "#DATA_DIR is the main folder where all your text files are stored.\n",
    "#\"**\" means “search in all subdirectories of this folder as well” (when recursive=True is used).\n",
    "#\"*.txt\" means match all files that end with .txt.\n",
    "\n",
    "#this will print the number of text files and the folder name in which all files are present\n",
    "print(f\"\\nFound {len(txt_files)} .txt files under: {DATA_DIR}\\n\")\n",
    "\n",
    "# show up to first 30 filenames (basename only)\n",
    "sample = [os.path.basename(p) for p in txt_files[:30]]\n",
    " #os.path.basename(path) extracts only the filename part of a full path.\n",
    "    #For example: os.path.basename(\"C:/GestureData/person1/gesture1.txt\")\n",
    "    # → \"gesture1.txt\"\n",
    "\n",
    "for i, name in enumerate(sample, 1):\n",
    "    print(f\"{i:2d}. {name}\") \n",
    "    #enumerate loops over the list but also gives you an index for each element.\n",
    "    #The 1 means the index starts at 1 (instead of the default 0).\n",
    "    #This is formatted printing.\n",
    "    #f\"{i:2d}\" means print the number i as a 2-digit integer, aligned nicely.\n",
    "    #This is just to visually inspect that your files were read correctly.\n",
    "\n",
    "# Save a file index for traceability\n",
    "index_df = pd.DataFrame({\"filepath\": txt_files, \"filename\": [os.path.basename(p) for p in txt_files]})\n",
    "out_index = os.path.join(DATA_DIR, \"file_index.csv\")\n",
    "index_df.to_csv(out_index, index=False)\n",
    "print(f\"\\nSaved file index to: {out_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3469aea0-69bd-4752-b3ea-314cdf0c6f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433b1e98-9dcc-4923-876a-52eca2b8dd0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be57a5d-c32f-4110-ba23-61fae1c5bbc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c76a8f3-d300-49ae-a99b-81aac440b9d9",
   "metadata": {},
   "source": [
    "# Parse Labels (Extract Gesture Names from Filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2498e13d-b1d3-4cec-b345-064e432e5ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            filepath                filename\n",
      "0  C:\\Users\\Talha\\Documents\\DataScience\\Data Mini...  afternoon_apurve_1.txt\n",
      "1  C:\\Users\\Talha\\Documents\\DataScience\\Data Mini...  afternoon_apurve_2.txt\n",
      "2  C:\\Users\\Talha\\Documents\\DataScience\\Data Mini...  afternoon_apurve_3.txt\n",
      "3  C:\\Users\\Talha\\Documents\\DataScience\\Data Mini...  afternoon_apurve_4.txt\n",
      "4  C:\\Users\\Talha\\Documents\\DataScience\\Data Mini...  afternoon_apurve_5.txt\n",
      "                                            filepath                filename  \\\n",
      "0  C:\\Users\\Talha\\Documents\\DataScience\\Data Mini...  afternoon_apurve_1.txt   \n",
      "1  C:\\Users\\Talha\\Documents\\DataScience\\Data Mini...  afternoon_apurve_2.txt   \n",
      "2  C:\\Users\\Talha\\Documents\\DataScience\\Data Mini...  afternoon_apurve_3.txt   \n",
      "3  C:\\Users\\Talha\\Documents\\DataScience\\Data Mini...  afternoon_apurve_4.txt   \n",
      "4  C:\\Users\\Talha\\Documents\\DataScience\\Data Mini...  afternoon_apurve_5.txt   \n",
      "\n",
      "       label  \n",
      "0  afternoon  \n",
      "1  afternoon  \n",
      "2  afternoon  \n",
      "3  afternoon  \n",
      "4  afternoon  \n",
      "Unique/Number of gesture labels:  ['afternoon' 'baby' 'big' 'born' 'bye' 'calendar' 'child' 'cloud' 'come'\n",
      " 'daily' 'dance' 'dark' 'day' 'enjoy' 'go' 'hello' 'home' 'love' 'my'\n",
      " 'name' 'no' 'rain' 'sorry' 'strong' 'study' 'thankyou' 'welcome' 'wind'\n",
      " 'yes' 'you']\n",
      "✅ Saved updated index with labels to file_index_with_labels.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#this will read all the files in the folder and subfolders and find the file_index.csv file by itself\n",
    "#print(glob.glob(\"**/file_index.csv\", recursive=True))\n",
    "\n",
    "file_index= pd.read_csv(\"combined/file_index.csv\")\n",
    "print(file_index.head())\n",
    "\n",
    "# Extract gesture label from the filename (before the first underscore)\n",
    "file_index['label']= file_index['filename'].apply(lambda x:x.split('_')[0])\n",
    "\n",
    "#check the first few rows to verify\n",
    "print(file_index.head(5))\n",
    "\n",
    "#check unique/number of gesture lables we have\n",
    "print(\"Unique/Number of gesture labels: \",file_index['label'].unique())\n",
    "\n",
    "# Save the updated file index with 3rd column name label\n",
    "file_index.to_csv(\"file_index_with_labels.csv\", index=False)\n",
    "print(\"✅ Saved updated index with labels to file_index_with_labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b32476-449d-4fda-bdf1-2d5703ccf967",
   "metadata": {},
   "source": [
    "# Sequence level Features Extraction from each text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "246e9d8e-a6f0-4762-8dda-e51ab4f442a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First File Path:  C:\\Users\\Talha\\Documents\\DataScience\\Data Mining\\Project\\DataMining-Coursework\\combined\\afternoon_apurve_1.txt\n",
      "Shape of Data:  (64, 60)\n",
      "Shape of feature_list:  (1, 240)\n",
      "   col1_mean:   col2_std:   col3_min:   col4_max:   col2_mean:   col3_std:   \\\n",
      "0     -0.38381    0.003073   -0.387078   -0.376956     0.673528     0.00205   \n",
      "\n",
      "   col4_min:   col5_max:   col3_mean:   col4_std:   ...  col60_min:   \\\n",
      "0    0.669575    0.678407     2.477884    0.001087  ...    -0.211996   \n",
      "\n",
      "   col61_max:   col59_mean:   col60_std:   col61_min:   col62_max:   \\\n",
      "0    -0.206404     -0.937998     0.000981    -0.940838    -0.935661   \n",
      "\n",
      "   col60_mean:   col61_std:   col62_min:   col63_max:   \n",
      "0      2.647377       0.0034     2.633385     2.651656  \n",
      "\n",
      "[1 rows x 240 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#this will read all the files in the folder and subfolders and find the file_index.csv file by itself\n",
    "#print(glob.glob(\"**/file_index_with_labels.csv\", recursive=True))\n",
    "\n",
    "txt_files= pd.read_csv(\"combined/file_index_with_labels.csv\")\n",
    "txt_files.head()\n",
    "\n",
    "#pick the first file path\n",
    "first_file_path= txt_files[\"filepath\"].iloc[0]\n",
    "print(\"First File Path: \", first_file_path)\n",
    "\n",
    "#load the first file as numpy array\n",
    "data= np.loadtxt(first_file_path)\n",
    "print(\"Shape of Data: \", data.shape)\n",
    "\n",
    "#Display few Frame level features\n",
    "data[:2]\n",
    "\n",
    "#Calculate the statistical level features for all 60 columns\n",
    "feature_dict={}\n",
    "\n",
    "for i in range(data.shape[1]):\n",
    "    feature_dict[f'col{i+1}_mean: ']= np.mean(data[:, i])\n",
    "    feature_dict[f'col{i+2}_std: ']= np.std(data[:,i])\n",
    "    feature_dict[f'col{i+3}_min: ']= np.min(data[:,i])\n",
    "    feature_dict[f'col{i+4}_max: ']= np.max(data[:,i])\n",
    "    \n",
    "\n",
    "\n",
    "#Convert this into DataFrame (1 row of feature)\n",
    "feature_df= pd.DataFrame([feature_dict])\n",
    "print(\"Shape of feature_list: \",feature_df.shape)\n",
    "print(feature_df.head(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c78330-30fc-422d-95bd-b5815787ac92",
   "metadata": {},
   "source": [
    "# Now Extract all files Sequence Level Gestures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb3c287a-a0de-4f8e-82de-8ff75239445d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m     features= extract_feature_from_file(file_path)\n\u001b[32m     30\u001b[39m     features[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m]= label\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[43mall_feature_dict\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m(features)\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# combine into one DataFrame\u001b[39;00m\n\u001b[32m     35\u001b[39m feature_df= pd.DataFrame(all_feature_dict)\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# Define feature extraction function to read text files\n",
    "\n",
    "def extract_feature_from_file(filepath):\n",
    "\n",
    "    df= pd.read_csv(filepath,header=None)\n",
    "    #This tells pandas not to treat the first row as column headers.\n",
    "    #Normally, pandas assumes the first line of a file contains the column names.\n",
    "    #But in our .txt files, every line is just numeric data (like sensor readings) — there are no headers.\n",
    "    feature_dict={}\n",
    "\n",
    "    for i in range(data.shape[1]):\n",
    "        feature_dict[f'col{i+1}_mean: ']= np.mean(data[:,i])\n",
    "        feature_dict[f'col{i+2}_std: ']= np.std(data[:,i])\n",
    "        feature_dict[f'col{i+3}_min: ']= np.min(data[:,i])\n",
    "        feature_dict[f'col{i+4}_max: ']= np.max(data[:,i])\n",
    "\n",
    "    return feature_dict\n",
    "\n",
    "# Loop through all gesture files and extract features\n",
    "all_feature_dict= []\n",
    "\n",
    "    #.iterrows() function lets us go row by row through that DataFrame.\n",
    "    #i → gives the index number (like 0, 1, 2, 3…)\n",
    "    #row → gives that row’s content (like a small dictionary of column names and values).\n",
    "for i,row in file_index.iterrows():\n",
    "        #We extract the full file path from that row so we can open that specific file.\n",
    "    file_path= row['filepath']\n",
    "    label= row['label']\n",
    "    features= extract_feature_from_file(file_path)\n",
    "    features['label']= label\n",
    "    all_feature_dict.append(features)\n",
    "\n",
    "\n",
    "# combine into one DataFrame\n",
    "feature_df= pd.DataFrame(all_feature_dict)\n",
    "\n",
    "# Save the resulting dataset\n",
    "output_path = os.path.join(\"combined\", \"sequence_level_features.csv\")\n",
    "features_df.to_csv(output_path, index=False)\n",
    "print(f\"✅ Feature extraction complete. Saved to: {output_path}\")\n",
    "print(f\"Shape of final dataset: {features_df.shape}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc68848-929c-4758-9685-d6db2611f1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env (conda)",
   "language": "python",
   "name": "my-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
