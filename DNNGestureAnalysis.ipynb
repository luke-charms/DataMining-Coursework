{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e270444f-3803-48ef-8fb5-22a8c3a50a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "982606bc-72ac-494f-bb67-34d362975b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\Talha\\Documents\\DataScience\\Data Mining\\Project\\DataMining-Coursework\n",
      "Checking DATA_DIR exists: C:\\Users\\Talha\\Documents\\DataScience\\Data Mining\\Project\\DataMining-Coursework\\combined\n",
      "\n",
      "Found 2700 .txt files under: C:\\Users\\Talha\\Documents\\DataScience\\Data Mining\\Project\\DataMining-Coursework\\combined\n",
      "\n",
      " 1. afternoon_apurve_1.txt\n",
      " 2. afternoon_apurve_2.txt\n",
      " 3. afternoon_apurve_3.txt\n",
      " 4. afternoon_apurve_4.txt\n",
      " 5. afternoon_apurve_5.txt\n",
      " 6. afternoon_apurve_6.txt\n",
      " 7. afternoon_apurve_7.txt\n",
      " 8. afternoon_apurve_8.txt\n",
      " 9. afternoon_apurve_9.txt\n",
      "10. afternoon_gautam_1.txt\n",
      "11. afternoon_gautam_2.txt\n",
      "12. afternoon_gautam_3.txt\n",
      "13. afternoon_gautam_4.txt\n",
      "14. afternoon_gautam_5.txt\n",
      "15. afternoon_gautam_6.txt\n",
      "16. afternoon_gautam_7.txt\n",
      "17. afternoon_gautam_8.txt\n",
      "18. afternoon_gautam_9.txt\n",
      "19. afternoon_mahendra_1.txt\n",
      "20. afternoon_mahendra_2.txt\n",
      "21. afternoon_mahendra_3.txt\n",
      "22. afternoon_mahendra_4.txt\n",
      "23. afternoon_mahendra_5.txt\n",
      "24. afternoon_mahendra_6.txt\n",
      "25. afternoon_mahendra_7.txt\n",
      "26. afternoon_mahendra_8.txt\n",
      "27. afternoon_mahendra_9.txt\n",
      "28. afternoon_parveen_1.txt\n",
      "29. afternoon_parveen_2.txt\n",
      "30. afternoon_parveen_3.txt\n",
      "\n",
      "Saved file index to: C:\\Users\\Talha\\Documents\\DataScience\\Data Mining\\Project\\DataMining-Coursework\\combined\\file_index.csv\n"
     ]
    }
   ],
   "source": [
    "# --- USER: set the folder that contains all your .txt files ---\n",
    "DATA_DIR = r\"C:\\Users\\Talha\\Documents\\DataScience\\Data Mining\\Project\\DataMining-Coursework\\combined\"\n",
    "\n",
    "# helpful checks\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Checking DATA_DIR exists:\", DATA_DIR)\n",
    "if not os.path.isdir(DATA_DIR):\n",
    "    raise NotADirectoryError(f\"Path does not exist or is not a directory: {DATA_DIR}\")\n",
    "\n",
    "# find all .txt files (recursive)\n",
    "txt_files = sorted(glob.glob(os.path.join(DATA_DIR, \"**\", \"*.txt\"), recursive=True))\n",
    "\n",
    "#os.path.join(DATA_DIR, \"**\", \"*.txt\")\n",
    "#os.path.join safely builds a file path that works on all operating systems (Windows, Linux, Mac).\n",
    "#DATA_DIR is the main folder where all your text files are stored.\n",
    "#\"**\" means â€œsearch in all subdirectories of this folder as wellâ€ (when recursive=True is used).\n",
    "#\"*.txt\" means match all files that end with .txt.\n",
    "\n",
    "#this will print the number of text files and the folder name in which all files are present\n",
    "print(f\"\\nFound {len(txt_files)} .txt files under: {DATA_DIR}\\n\")\n",
    "\n",
    "# show up to first 30 filenames (basename only)\n",
    "sample = [os.path.basename(p) for p in txt_files[:30]]\n",
    " #os.path.basename(path) extracts only the filename part of a full path.\n",
    "    #For example: os.path.basename(\"C:/GestureData/person1/gesture1.txt\")\n",
    "    # â†’ \"gesture1.txt\"\n",
    "\n",
    "for i, name in enumerate(sample, 1):\n",
    "    print(f\"{i:2d}. {name}\") \n",
    "    #enumerate loops over the list but also gives you an index for each element.\n",
    "    #The 1 means the index starts at 1 (instead of the default 0).\n",
    "    #This is formatted printing.\n",
    "    #f\"{i:2d}\" means print the number i as a 2-digit integer, aligned nicely.\n",
    "    #This is just to visually inspect that your files were read correctly.\n",
    "\n",
    "# Save a file index for traceability\n",
    "index_df = pd.DataFrame({\"filepath\": txt_files, \"filename\": [os.path.basename(p) for p in txt_files]})\n",
    "out_index = os.path.join(DATA_DIR, \"file_index.csv\")\n",
    "index_df.to_csv(out_index, index=False)\n",
    "print(f\"\\nSaved file index to: {out_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c76a8f3-d300-49ae-a99b-81aac440b9d9",
   "metadata": {},
   "source": [
    "# Parse Labels (Extract Gesture Names from Filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2498e13d-b1d3-4cec-b345-064e432e5ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            filepath                filename\n",
      "0  C:\\Users\\Talha\\Documents\\DataScience\\Data Mini...  afternoon_apurve_1.txt\n",
      "1  C:\\Users\\Talha\\Documents\\DataScience\\Data Mini...  afternoon_apurve_2.txt\n",
      "2  C:\\Users\\Talha\\Documents\\DataScience\\Data Mini...  afternoon_apurve_3.txt\n",
      "3  C:\\Users\\Talha\\Documents\\DataScience\\Data Mini...  afternoon_apurve_4.txt\n",
      "4  C:\\Users\\Talha\\Documents\\DataScience\\Data Mini...  afternoon_apurve_5.txt\n",
      "                                            filepath                filename  \\\n",
      "0  C:\\Users\\Talha\\Documents\\DataScience\\Data Mini...  afternoon_apurve_1.txt   \n",
      "1  C:\\Users\\Talha\\Documents\\DataScience\\Data Mini...  afternoon_apurve_2.txt   \n",
      "2  C:\\Users\\Talha\\Documents\\DataScience\\Data Mini...  afternoon_apurve_3.txt   \n",
      "3  C:\\Users\\Talha\\Documents\\DataScience\\Data Mini...  afternoon_apurve_4.txt   \n",
      "4  C:\\Users\\Talha\\Documents\\DataScience\\Data Mini...  afternoon_apurve_5.txt   \n",
      "\n",
      "       label  \n",
      "0  afternoon  \n",
      "1  afternoon  \n",
      "2  afternoon  \n",
      "3  afternoon  \n",
      "4  afternoon  \n",
      "Unique/Number of gesture labels:  ['afternoon' 'baby' 'big' 'born' 'bye' 'calendar' 'child' 'cloud' 'come'\n",
      " 'daily' 'dance' 'dark' 'day' 'enjoy' 'go' 'hello' 'home' 'love' 'my'\n",
      " 'name' 'no' 'rain' 'sorry' 'strong' 'study' 'thankyou' 'welcome' 'wind'\n",
      " 'yes' 'you']\n",
      "âœ… Saved updated index with labels to file_index_with_labels.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#this will read all the files in the folder and subfolders and find the file_index.csv file by itself\n",
    "#print(glob.glob(\"**/file_index.csv\", recursive=True))\n",
    "\n",
    "file_index= pd.read_csv(\"combined/file_index.csv\")\n",
    "print(file_index.head())\n",
    "\n",
    "# Extract gesture label from the filename (before the first underscore)\n",
    "file_index['label']= file_index['filename'].apply(lambda x:x.split('_')[0])\n",
    "\n",
    "#check the first few rows to verify\n",
    "print(file_index.head(5))\n",
    "\n",
    "#check unique/number of gesture lables we have\n",
    "print(\"Unique/Number of gesture labels: \",file_index['label'].unique())\n",
    "\n",
    "# Save the updated file index with 3rd column name label\n",
    "file_index.to_csv(\"file_index_with_labels.csv\", index=False)\n",
    "print(\"âœ… Saved updated index with labels to file_index_with_labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b32476-449d-4fda-bdf1-2d5703ccf967",
   "metadata": {},
   "source": [
    "# Sequence level Features Extraction from each text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "246e9d8e-a6f0-4762-8dda-e51ab4f442a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First File Path:  C:\\Users\\Talha\\Documents\\DataScience\\Data Mining\\Project\\DataMining-Coursework\\combined\\afternoon_apurve_1.txt\n",
      "Shape of Data:  (64, 60)\n",
      "Shape of feature_list:  (1, 240)\n",
      "   col1_mean:   col2_std:   col3_min:   col4_max:   col2_mean:   col3_std:   \\\n",
      "0     -0.38381    0.003073   -0.387078   -0.376956     0.673528     0.00205   \n",
      "\n",
      "   col4_min:   col5_max:   col3_mean:   col4_std:   ...  col60_min:   \\\n",
      "0    0.669575    0.678407     2.477884    0.001087  ...    -0.211996   \n",
      "\n",
      "   col61_max:   col59_mean:   col60_std:   col61_min:   col62_max:   \\\n",
      "0    -0.206404     -0.937998     0.000981    -0.940838    -0.935661   \n",
      "\n",
      "   col60_mean:   col61_std:   col62_min:   col63_max:   \n",
      "0      2.647377       0.0034     2.633385     2.651656  \n",
      "\n",
      "[1 rows x 240 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#this will read all the files in the folder and subfolders and find the file_index.csv file by itself\n",
    "#print(glob.glob(\"**/file_index_with_labels.csv\", recursive=True))\n",
    "\n",
    "txt_files= pd.read_csv(\"combined/file_index_with_labels.csv\")\n",
    "txt_files.head()\n",
    "\n",
    "#pick the first file path\n",
    "first_file_path= txt_files[\"filepath\"].iloc[0]\n",
    "print(\"First File Path: \", first_file_path)\n",
    "\n",
    "#load the first file as numpy array\n",
    "data= np.loadtxt(first_file_path)\n",
    "print(\"Shape of Data: \", data.shape)\n",
    "\n",
    "#Display few Frame level features\n",
    "data[:2]\n",
    "\n",
    "#Calculate the statistical level features for all 60 columns\n",
    "feature_dict={}\n",
    "\n",
    "for i in range(data.shape[1]):\n",
    "    feature_dict[f'col{i+1}_mean: ']= np.mean(data[:, i])\n",
    "    feature_dict[f'col{i+2}_std: ']= np.std(data[:,i])\n",
    "    feature_dict[f'col{i+3}_min: ']= np.min(data[:,i])\n",
    "    feature_dict[f'col{i+4}_max: ']= np.max(data[:,i])\n",
    "    \n",
    "\n",
    "\n",
    "#Convert this into DataFrame (1 row of feature)\n",
    "feature_df= pd.DataFrame([feature_dict])\n",
    "print(\"Shape of feature_list: \",feature_df.shape)\n",
    "print(feature_df.head(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c78330-30fc-422d-95bd-b5815787ac92",
   "metadata": {},
   "source": [
    "# Now Extract all files Sequence Level Gestures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb3c287a-a0de-4f8e-82de-8ff75239445d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Feature extraction complete. Saved to: combined\\sequence_level_features_with_labels.csv\n",
      "Shape of final dataset: (2700, 241)\n"
     ]
    }
   ],
   "source": [
    "# Define feature extraction function to read text files\n",
    "def extract_feature_from_file(filepath):\n",
    "    # Read the file with whitespace as separator\n",
    "    df = pd.read_csv(filepath, header=None, sep=r'\\s+')\n",
    "    \n",
    "    # Keep only numeric columns (just in case)\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # If no numeric columns, skip file\n",
    "    if numeric_df.shape[1] == 0:\n",
    "        return None\n",
    "    \n",
    "    # Compute min, max, mean, std for each column\n",
    "    features = numeric_df.agg(['min', 'max', 'mean', 'std']).values.flatten()\n",
    "    \n",
    "    # Here we are manually finding the statistics values\n",
    "    # for i in range(data.shape[1]):\n",
    "    #     feature_dict[f'col{i+1}_mean: ']= np.mean(data[:,i])\n",
    "    #     feature_dict[f'col{i+2}_std: ']= np.std(data[:,i])\n",
    "    #     feature_dict[f'col{i+3}_min: ']= np.min(data[:,i])\n",
    "    #     feature_dict[f'col{i+4}_max: ']= np.max(data[:,i])\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "# Loop through all gesture files and extract features\n",
    "all_feature_dict = []\n",
    "all_labels_dict = []\n",
    "\n",
    "for i, row in file_index.iterrows():\n",
    "    file_path = row['filepath']\n",
    "    label = row['label']\n",
    "    \n",
    "    feature = extract_feature_from_file(file_path)\n",
    "    \n",
    "    if feature is None:\n",
    "        print(f\"âš ï¸ Skipping file (no numeric data): {file_path}\")\n",
    "        continue\n",
    "    \n",
    "    all_feature_dict.append(feature)\n",
    "    all_labels_dict.append(label)\n",
    "\n",
    "\n",
    "# Generate column names dynamically\n",
    "num_cols = pd.read_csv(file_index['filepath'].iloc[0], header=None, sep=r'\\s+').shape[1]\n",
    "feature_names = []\n",
    "\n",
    "for col in range(num_cols):\n",
    "    feature_names.extend([\n",
    "        f'col{col+1}_min',\n",
    "        f'col{col+1}_max',\n",
    "        f'col{col+1}_mean',\n",
    "        f'col{col+1}_std'\n",
    "    ])\n",
    "\n",
    "\n",
    "# Combine into one DataFrame\n",
    "feature_df = pd.DataFrame(all_feature_dict, columns=feature_names)\n",
    "feature_df.insert(0, 'label', all_labels_dict)  # insert at the first column\n",
    "\n",
    "\n",
    "\n",
    "# Save the resulting dataset\n",
    "output_path = os.path.join(\"combined\", \"sequence_level_features_with_labels.csv\")\n",
    "feature_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"âœ… Feature extraction complete. Saved to: {output_path}\")\n",
    "print(f\"Shape of final dataset: {feature_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026c3241-9232-48a9-980f-dbc6246c609f",
   "metadata": {},
   "source": [
    "# Building, Training and Evaluate Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31312fb7-7dae-4995-95f0-a8bd17f4eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "feature_df= pd.read_csv(\"combined/sequence_level_features_with_labels.csv\")\n",
    "\n",
    "#it will extract all columns except label\n",
    "X= feature_df.drop('label', axis=1)\n",
    "\n",
    "# it will extract only label column as target column\n",
    "y= feature_df['label']\n",
    "\n",
    "# Encode gesture labels (text â†’ numbers)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Normalize features (important for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# split data into training and test\n",
    "X_train, X_test, y_train, y_test= train_test_split(\n",
    "    X_scaled,y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Convert labels to one-hot encoding (for multi-class classification)\n",
    "y_train_encoded = to_categorical(y_train)\n",
    "y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "# Reshape features for sequential models (add 1 time step)\n",
    "X_train_seq = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test_seq = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "num_classes = y_train_encoded.shape[1]\n",
    "input_shape = (1, X_train.shape[1])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c105d8-405d-4839-a6ea-ff45c681665a",
   "metadata": {},
   "source": [
    "# Building Model(RNN, LSTM, BiLSTM, GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a52c88-519f-4002-b0dc-a1f21d3dadba",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26187ab9-91ce-498b-b6ff-04e79d02fd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Talha\\anaconda3\\envs\\my-env\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - accuracy: 0.1111 - loss: 3.1402 - val_accuracy: 0.2245 - val_loss: 2.6082\n",
      "Epoch 2/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3808 - loss: 2.1567 - val_accuracy: 0.4560 - val_loss: 1.8725\n",
      "Epoch 3/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5758 - loss: 1.5297 - val_accuracy: 0.5880 - val_loss: 1.3986\n",
      "Epoch 4/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6684 - loss: 1.1483 - val_accuracy: 0.6667 - val_loss: 1.0981\n",
      "Epoch 5/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7407 - loss: 0.9154 - val_accuracy: 0.7245 - val_loss: 0.8909\n",
      "Epoch 6/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7911 - loss: 0.7430 - val_accuracy: 0.7940 - val_loss: 0.7430\n",
      "Epoch 7/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8206 - loss: 0.6172 - val_accuracy: 0.7986 - val_loss: 0.6624\n",
      "Epoch 8/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8449 - loss: 0.5418 - val_accuracy: 0.8310 - val_loss: 0.5770\n",
      "Epoch 9/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8542 - loss: 0.4892 - val_accuracy: 0.8403 - val_loss: 0.5187\n",
      "Epoch 10/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8854 - loss: 0.4206 - val_accuracy: 0.8333 - val_loss: 0.5117\n",
      "Epoch 11/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8929 - loss: 0.3825 - val_accuracy: 0.8519 - val_loss: 0.4612\n",
      "Epoch 12/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9016 - loss: 0.3439 - val_accuracy: 0.8472 - val_loss: 0.4567\n",
      "Epoch 13/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8976 - loss: 0.3266 - val_accuracy: 0.8634 - val_loss: 0.4137\n",
      "Epoch 14/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9155 - loss: 0.2844 - val_accuracy: 0.8588 - val_loss: 0.3997\n",
      "Epoch 15/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9149 - loss: 0.2678 - val_accuracy: 0.8704 - val_loss: 0.3804\n",
      "Epoch 16/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9242 - loss: 0.2630 - val_accuracy: 0.8773 - val_loss: 0.3506\n",
      "Epoch 17/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9369 - loss: 0.2297 - val_accuracy: 0.8843 - val_loss: 0.3578\n",
      "Epoch 18/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9427 - loss: 0.2170 - val_accuracy: 0.8866 - val_loss: 0.3382\n",
      "Epoch 19/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9410 - loss: 0.2007 - val_accuracy: 0.8935 - val_loss: 0.3149\n",
      "Epoch 20/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9404 - loss: 0.1907 - val_accuracy: 0.9005 - val_loss: 0.3080\n",
      "Epoch 21/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9485 - loss: 0.1857 - val_accuracy: 0.8935 - val_loss: 0.3183\n",
      "Epoch 22/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9647 - loss: 0.1515 - val_accuracy: 0.9120 - val_loss: 0.2790\n",
      "Epoch 23/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9566 - loss: 0.1523 - val_accuracy: 0.8935 - val_loss: 0.2862\n",
      "Epoch 24/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9635 - loss: 0.1425 - val_accuracy: 0.8912 - val_loss: 0.2899\n",
      "Epoch 25/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9606 - loss: 0.1467 - val_accuracy: 0.9028 - val_loss: 0.2579\n",
      "Epoch 26/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9664 - loss: 0.1286 - val_accuracy: 0.9236 - val_loss: 0.2722\n",
      "Epoch 27/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9595 - loss: 0.1325 - val_accuracy: 0.9190 - val_loss: 0.2620\n",
      "Epoch 28/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9641 - loss: 0.1320 - val_accuracy: 0.8958 - val_loss: 0.2654\n",
      "Epoch 29/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9728 - loss: 0.1162 - val_accuracy: 0.9167 - val_loss: 0.2643\n",
      "Epoch 30/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9728 - loss: 0.1043 - val_accuracy: 0.9213 - val_loss: 0.2435\n",
      "Epoch 31/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9705 - loss: 0.1083 - val_accuracy: 0.9236 - val_loss: 0.2575\n",
      "Epoch 32/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9705 - loss: 0.1047 - val_accuracy: 0.9167 - val_loss: 0.2618\n",
      "Epoch 33/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9728 - loss: 0.0990 - val_accuracy: 0.9074 - val_loss: 0.2492\n",
      "Epoch 34/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9745 - loss: 0.0901 - val_accuracy: 0.9213 - val_loss: 0.2465\n",
      "Epoch 35/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9699 - loss: 0.0995 - val_accuracy: 0.9306 - val_loss: 0.2291\n",
      "Epoch 36/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9774 - loss: 0.0815 - val_accuracy: 0.9236 - val_loss: 0.2417\n",
      "Epoch 37/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9838 - loss: 0.0730 - val_accuracy: 0.9282 - val_loss: 0.2526\n",
      "Epoch 38/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9809 - loss: 0.0782 - val_accuracy: 0.9306 - val_loss: 0.2329\n",
      "Epoch 39/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9861 - loss: 0.0657 - val_accuracy: 0.9306 - val_loss: 0.2450\n",
      "Epoch 40/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9826 - loss: 0.0717 - val_accuracy: 0.9144 - val_loss: 0.2385\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "rnn_model = Sequential([\n",
    "    SimpleRNN(128, input_shape=input_shape, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "rnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history_rnn = rnn_model.fit(\n",
    "    X_train_seq, y_train_encoded,\n",
    "    validation_split=0.2,\n",
    "    epochs=50, batch_size=32,\n",
    "    callbacks=[early_stop], verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7e77e0-4c11-4aa1-872c-67b9b9735c17",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "243aab0f-d538-4692-9e56-6ce3c526422f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.1562 - loss: 3.1894 - val_accuracy: 0.2431 - val_loss: 2.9243\n",
      "Epoch 2/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.3652 - loss: 2.4347 - val_accuracy: 0.4051 - val_loss: 2.1687\n",
      "Epoch 3/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5208 - loss: 1.7348 - val_accuracy: 0.5324 - val_loss: 1.6008\n",
      "Epoch 4/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6765 - loss: 1.2532 - val_accuracy: 0.6250 - val_loss: 1.2516\n",
      "Epoch 5/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7355 - loss: 0.9667 - val_accuracy: 0.7454 - val_loss: 1.0011\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    LSTM(128, input_shape=input_shape, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_lstm = lstm_model.fit(\n",
    "    X_train_seq, y_train_encoded,\n",
    "    validation_split=0.2,\n",
    "    epochs=50, batch_size=32,\n",
    "    callbacks=[early_stop], verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a03b31-db0a-4b81-8e9b-aeab04c88ec3",
   "metadata": {},
   "source": [
    "# BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6f21932-309d-4eda-963a-a4e4b7e3e697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Talha\\anaconda3\\envs\\my-env\\Lib\\site-packages\\keras\\src\\layers\\rnn\\bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - accuracy: 0.1765 - loss: 3.0434 - val_accuracy: 0.2500 - val_loss: 2.6532\n",
      "Epoch 2/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4381 - loss: 2.0708 - val_accuracy: 0.4676 - val_loss: 1.8032\n",
      "Epoch 3/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6227 - loss: 1.3526 - val_accuracy: 0.6644 - val_loss: 1.2566\n",
      "Epoch 4/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7616 - loss: 0.9444 - val_accuracy: 0.7106 - val_loss: 0.9953\n",
      "Epoch 5/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8218 - loss: 0.6995 - val_accuracy: 0.7894 - val_loss: 0.7470\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "bilstm_model = Sequential([\n",
    "    Bidirectional(LSTM(128, return_sequences=False), input_shape=input_shape),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "bilstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_bilstm = bilstm_model.fit(\n",
    "    X_train_seq, y_train_encoded,\n",
    "    validation_split=0.2,\n",
    "    epochs=50, batch_size=32,\n",
    "    callbacks=[early_stop], verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d53140-2e38-4240-a613-5f78b0a437f1",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a32e8fc3-7042-4628-81c9-89345140dd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.1591 - loss: 3.1304 - val_accuracy: 0.2500 - val_loss: 2.7476\n",
      "Epoch 2/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3779 - loss: 2.2301 - val_accuracy: 0.4236 - val_loss: 1.9701\n",
      "Epoch 3/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5758 - loss: 1.5524 - val_accuracy: 0.6296 - val_loss: 1.4248\n",
      "Epoch 4/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6701 - loss: 1.1536 - val_accuracy: 0.7384 - val_loss: 1.0706\n",
      "Epoch 5/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7552 - loss: 0.8971 - val_accuracy: 0.7593 - val_loss: 0.8891\n",
      "Epoch 6/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8050 - loss: 0.7305 - val_accuracy: 0.8079 - val_loss: 0.7293\n",
      "Epoch 7/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8426 - loss: 0.5906 - val_accuracy: 0.8241 - val_loss: 0.6353\n",
      "Epoch 8/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8669 - loss: 0.4959 - val_accuracy: 0.8495 - val_loss: 0.5547\n",
      "Epoch 9/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8877 - loss: 0.4211 - val_accuracy: 0.8356 - val_loss: 0.5200\n",
      "Epoch 10/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8947 - loss: 0.3637 - val_accuracy: 0.8819 - val_loss: 0.4387\n",
      "Epoch 11/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9225 - loss: 0.3048 - val_accuracy: 0.8889 - val_loss: 0.3963\n",
      "Epoch 12/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9265 - loss: 0.2724 - val_accuracy: 0.8704 - val_loss: 0.3912\n",
      "Epoch 13/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9340 - loss: 0.2438 - val_accuracy: 0.8866 - val_loss: 0.3691\n",
      "Epoch 14/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9375 - loss: 0.2226 - val_accuracy: 0.8958 - val_loss: 0.3300\n",
      "Epoch 15/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9508 - loss: 0.1865 - val_accuracy: 0.9028 - val_loss: 0.3149\n",
      "Epoch 16/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9659 - loss: 0.1558 - val_accuracy: 0.9074 - val_loss: 0.3066\n",
      "Epoch 17/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9572 - loss: 0.1675 - val_accuracy: 0.9190 - val_loss: 0.2847\n",
      "Epoch 18/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9688 - loss: 0.1382 - val_accuracy: 0.9120 - val_loss: 0.2726\n",
      "Epoch 19/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9647 - loss: 0.1479 - val_accuracy: 0.9097 - val_loss: 0.2678\n",
      "Epoch 20/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9670 - loss: 0.1352 - val_accuracy: 0.9306 - val_loss: 0.2523\n",
      "Epoch 21/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9618 - loss: 0.1277 - val_accuracy: 0.9282 - val_loss: 0.2320\n",
      "Epoch 22/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9763 - loss: 0.1148 - val_accuracy: 0.9120 - val_loss: 0.2377\n",
      "Epoch 23/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9774 - loss: 0.0955 - val_accuracy: 0.9352 - val_loss: 0.2188\n",
      "Epoch 24/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9821 - loss: 0.0825 - val_accuracy: 0.9282 - val_loss: 0.2330\n",
      "Epoch 25/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9855 - loss: 0.0765 - val_accuracy: 0.9329 - val_loss: 0.2119\n",
      "Epoch 26/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9786 - loss: 0.0802 - val_accuracy: 0.9282 - val_loss: 0.2200\n",
      "Epoch 27/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9826 - loss: 0.0754 - val_accuracy: 0.9190 - val_loss: 0.2461\n",
      "Epoch 28/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9890 - loss: 0.0715 - val_accuracy: 0.9259 - val_loss: 0.2197\n",
      "Epoch 29/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9826 - loss: 0.0694 - val_accuracy: 0.9468 - val_loss: 0.2128\n",
      "Epoch 30/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9803 - loss: 0.0737 - val_accuracy: 0.9444 - val_loss: 0.1833\n",
      "Epoch 31/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9925 - loss: 0.0497 - val_accuracy: 0.9444 - val_loss: 0.1840\n",
      "Epoch 32/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9936 - loss: 0.0497 - val_accuracy: 0.9468 - val_loss: 0.2042\n",
      "Epoch 33/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9855 - loss: 0.0578 - val_accuracy: 0.9398 - val_loss: 0.2127\n",
      "Epoch 34/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9815 - loss: 0.0592 - val_accuracy: 0.9306 - val_loss: 0.1970\n",
      "Epoch 35/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9861 - loss: 0.0589 - val_accuracy: 0.9398 - val_loss: 0.1976\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "#  Define timesteps and features based on your X_train shape\n",
    "timesteps = 1\n",
    "features = X_train.shape[1]\n",
    "num_classes = y_train_encoded.shape[1]\n",
    "\n",
    "#  Reshape your input data for GRU (RNN models require 3D input)\n",
    "X_train_seq = X_train.reshape((X_train.shape[0], timesteps, features))\n",
    "X_test_seq = X_test.reshape((X_test.shape[0], timesteps, features))\n",
    "\n",
    "#  Early stopping to prevent overfitting\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "#  Define GRU model\n",
    "gru_model = Sequential([\n",
    "    GRU(128, input_shape=(timesteps, features), return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "#  Compile model\n",
    "gru_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#  Train the model\n",
    "history_gru = gru_model.fit(\n",
    "    X_train_seq, y_train_encoded,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5599608a-38be-469e-ac68-8d6fb8e18a0c",
   "metadata": {},
   "source": [
    "# Accuracy, Precision, Recall, F1 Score for each Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b80ef575-d0fb-4f2e-9bc6-48678041ad7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:\n",
      "RNN:    0.9074\n",
      "LSTM:   0.2833\n",
      "BiLSTM: 0.2759\n",
      "GRU:    0.9185\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each trained model on test data\n",
    "rnn_test_loss, rnn_test_acc = rnn_model.evaluate(X_test_seq, y_test_encoded, verbose=0)\n",
    "lstm_test_loss, lstm_test_acc = lstm_model.evaluate(X_test_seq, y_test_encoded, verbose=0)\n",
    "bilstm_test_loss, bilstm_test_acc = bilstm_model.evaluate(X_test_seq, y_test_encoded, verbose=0)\n",
    "gru_test_loss, gru_test_acc = gru_model.evaluate(X_test_seq, y_test_encoded, verbose=0)\n",
    "\n",
    "print(\"Test Accuracy:\")\n",
    "print(f\"RNN:    {rnn_test_acc:.4f}\")\n",
    "print(f\"LSTM:   {lstm_test_acc:.4f}\")\n",
    "print(f\"BiLSTM: {bilstm_test_acc:.4f}\")\n",
    "print(f\"GRU:    {gru_test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd6b71c1-83ae-4f46-9362-5717d4a21302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "\n",
      "ğŸ“Š RNN Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        18\n",
      "           1       1.00      1.00      1.00        18\n",
      "           2       1.00      0.83      0.91        18\n",
      "           3       1.00      1.00      1.00        18\n",
      "           4       0.81      0.94      0.87        18\n",
      "           5       0.95      1.00      0.97        18\n",
      "           6       1.00      0.67      0.80        18\n",
      "           7       0.81      0.94      0.87        18\n",
      "           8       1.00      0.94      0.97        18\n",
      "           9       0.82      1.00      0.90        18\n",
      "          10       1.00      0.89      0.94        18\n",
      "          11       1.00      0.83      0.91        18\n",
      "          12       1.00      0.94      0.97        18\n",
      "          13       1.00      0.83      0.91        18\n",
      "          14       1.00      0.89      0.94        18\n",
      "          15       1.00      0.94      0.97        18\n",
      "          16       0.89      0.94      0.92        18\n",
      "          17       0.89      0.94      0.92        18\n",
      "          18       0.76      0.89      0.82        18\n",
      "          19       0.94      0.89      0.91        18\n",
      "          20       0.94      0.83      0.88        18\n",
      "          21       0.90      1.00      0.95        18\n",
      "          22       1.00      0.78      0.88        18\n",
      "          23       0.95      1.00      0.97        18\n",
      "          24       0.86      1.00      0.92        18\n",
      "          25       0.67      0.89      0.76        18\n",
      "          26       0.82      1.00      0.90        18\n",
      "          27       0.95      1.00      0.97        18\n",
      "          28       0.90      1.00      0.95        18\n",
      "          29       0.64      0.39      0.48        18\n",
      "\n",
      "    accuracy                           0.91       540\n",
      "   macro avg       0.91      0.91      0.91       540\n",
      "weighted avg       0.91      0.91      0.91       540\n",
      "\n",
      "\n",
      "RNN Confusion Matrix:\n",
      "[[18  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0 18  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0 15  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   2  0  0  0  0  0]\n",
      " [ 0  0  0 18  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 1  0  0  0 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 18  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 12  0  0  3  0  0  0  0  0  0  0  0  1  0  0  0  0  0\n",
      "   0  0  2  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 17  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 17  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 18  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  2  0  0 16  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  0  0 15  0  0  0  0  1  0  0  0  0  0  0  1\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0 17  0  0  0  0  1  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0  0  0  0 15  0  0  0  0  0  0  0  1  0  0\n",
      "   0  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 16  0  0  1  0  0  0  0  0  0\n",
      "   0  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 17  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 17  0  0  0  0  1  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 17  0  0  0  0  0  0\n",
      "   1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 16  0  1  0  0  0\n",
      "   0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 16  0  0  0  0\n",
      "   0  2  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1 15  0  0  0\n",
      "   0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 18  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0 14  0\n",
      "   0  0  1  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 18\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  18  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0 16  0  0  0  2]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0 18  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0 18  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0 18  0]\n",
      " [ 0  0  0  0  4  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0\n",
      "   0  4  0  0  2  7]]\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "\n",
      "ğŸ“Š LSTM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.78      0.43        18\n",
      "           1       0.41      0.61      0.49        18\n",
      "           2       1.00      0.22      0.36        18\n",
      "           3       0.00      0.00      0.00        18\n",
      "           4       0.31      0.44      0.36        18\n",
      "           5       0.00      0.00      0.00        18\n",
      "           6       0.09      0.11      0.10        18\n",
      "           7       0.00      0.00      0.00        18\n",
      "           8       0.50      0.06      0.10        18\n",
      "           9       0.50      0.17      0.25        18\n",
      "          10       0.23      1.00      0.37        18\n",
      "          11       0.58      0.61      0.59        18\n",
      "          12       0.50      0.22      0.31        18\n",
      "          13       1.00      0.06      0.11        18\n",
      "          14       0.56      0.28      0.37        18\n",
      "          15       0.33      0.06      0.10        18\n",
      "          16       0.00      0.00      0.00        18\n",
      "          17       0.10      0.06      0.07        18\n",
      "          18       0.32      0.33      0.32        18\n",
      "          19       0.28      0.44      0.34        18\n",
      "          20       0.13      0.33      0.18        18\n",
      "          21       0.24      0.67      0.36        18\n",
      "          22       0.33      0.06      0.10        18\n",
      "          23       0.50      0.06      0.10        18\n",
      "          24       0.14      0.11      0.12        18\n",
      "          25       0.22      0.11      0.15        18\n",
      "          26       0.00      0.00      0.00        18\n",
      "          27       0.38      1.00      0.55        18\n",
      "          28       0.26      0.67      0.37        18\n",
      "          29       0.20      0.06      0.09        18\n",
      "\n",
      "    accuracy                           0.28       540\n",
      "   macro avg       0.31      0.28      0.22       540\n",
      "weighted avg       0.31      0.28      0.22       540\n",
      "\n",
      "\n",
      "LSTM Confusion Matrix:\n",
      "[[14  0  0  0  1  0  0  0  0  0  2  0  0  0  0  1  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0 11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  1  0  6  0  0]\n",
      " [ 0  0  4  0  0  0  0  0  0  0 11  0  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "   1  0  0  1  0  0]\n",
      " [ 0  6  0  0  0  0  0  0  0  0  0  1  0  0  1  0  0  0  0  0  0  1  0  0\n",
      "   0  2  0  7  0  0]\n",
      " [ 2  0  0  0  8  0  4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  4  0]\n",
      " [ 0  3  0  0  0  0  0  0  0  0  0  1  0  0  0  0  2  1  0  0  0  8  0  0\n",
      "   0  3  0  0  0  0]\n",
      " [ 1  0  0  0  1  0  2  0  0  1  0  0  0  0  0  0  0  0  0  4  9  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 15  1  0  0  0  0  1  0  0  0  0  1  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  1  1  0  0  0  0  0  1  0  0  0  2  1  0  1  0\n",
      "   0  0  0  0  8  2]\n",
      " [ 0  2  0  0  0  0  4  0  0  3  0  0  0  0  0  0  0  0  0  4  3  1  0  0\n",
      "   0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 18  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  3 11  0  0  1  0  0  0  0  0  0  3  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0  1  0  0  2  1  4  0  0  0  0  2  0  0  0  4  0  0\n",
      "   2  0  0  1  0  0]\n",
      " [ 1  1  0  0  0  0  1  0  0  0  3  0  0  1  0  0  0  1  0  0  2  4  0  1\n",
      "   0  0  0  0  3  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  2  0  0  0  5  0  0  2  0  0  0  4  0  0\n",
      "   0  0  0  5  0  0]\n",
      " [ 8  0  0  0  7  0  2  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  8  3  0  0  1  0  0  0  0  0  0  3  0  0\n",
      "   0  0  0  3  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  1  0  0  0  3  0  0\n",
      "   5  0  0  7  0  0]\n",
      " [ 1  0  0  0  1  0  1  0  0  0  0  0  0  0  0  0  0  0  6  1  4  0  0  0\n",
      "   0  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  1  8  7  0  0  0\n",
      "   0  0  0  0  1  0]\n",
      " [ 1  0  0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0  0  5  6  0  0  0\n",
      "   0  0  1  0  1  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  5  1  0  0  0  0  0  0  0  0  0 12  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 1  0  0  0  1  0  1  0  0  0  0  0  0  0  0  0  0  0  8  3  0  1  1  0\n",
      "   0  0  0  0  2  0]\n",
      " [ 7  0  0  0  1  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  1\n",
      "   1  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  1  0  2  0  2  0  0  0  0  3  0  0  2  4  1  0\n",
      "   2  0  0  0  0  0]\n",
      " [ 4  0  0  0  3  0  2  0  0  0  0  0  0  0  0  0  0  0  0  1  3  0  0  0\n",
      "   2  2  0  0  1  0]\n",
      " [ 0  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  1  7  0  0  0\n",
      "   1  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0 18  0  0]\n",
      " [ 4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "   0  0  0  0 12  1]\n",
      " [ 2  0  0  0  2  0  2  0  0  0  0  0  0  0  0  0  0  0  2  0  2  0  0  0\n",
      "   0  0  0  0  7  1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Talha\\anaconda3\\envs\\my-env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\Talha\\anaconda3\\envs\\my-env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\Talha\\anaconda3\\envs\\my-env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step\n",
      "\n",
      "ğŸ“Š BiLSTM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.94      0.31        18\n",
      "           1       0.36      0.50      0.42        18\n",
      "           2       0.80      0.22      0.35        18\n",
      "           3       0.15      0.50      0.23        18\n",
      "           4       0.08      0.39      0.13        18\n",
      "           5       0.60      0.17      0.26        18\n",
      "           6       0.00      0.00      0.00        18\n",
      "           7       0.00      0.00      0.00        18\n",
      "           8       0.22      0.11      0.15        18\n",
      "           9       0.36      0.28      0.31        18\n",
      "          10       0.22      1.00      0.36        18\n",
      "          11       0.54      0.83      0.65        18\n",
      "          12       0.00      0.00      0.00        18\n",
      "          13       0.45      0.28      0.34        18\n",
      "          14       0.33      0.06      0.10        18\n",
      "          15       0.00      0.00      0.00        18\n",
      "          16       0.00      0.00      0.00        18\n",
      "          17       0.09      0.06      0.07        18\n",
      "          18       0.23      0.33      0.27        18\n",
      "          19       0.48      0.67      0.56        18\n",
      "          20       0.00      0.00      0.00        18\n",
      "          21       0.80      0.67      0.73        18\n",
      "          22       0.00      0.00      0.00        18\n",
      "          23       0.50      0.06      0.10        18\n",
      "          24       1.00      0.22      0.36        18\n",
      "          25       0.00      0.00      0.00        18\n",
      "          26       1.00      0.06      0.11        18\n",
      "          27       0.81      0.94      0.87        18\n",
      "          28       0.00      0.00      0.00        18\n",
      "          29       0.00      0.00      0.00        18\n",
      "\n",
      "    accuracy                           0.28       540\n",
      "   macro avg       0.31      0.28      0.22       540\n",
      "weighted avg       0.31      0.28      0.22       540\n",
      "\n",
      "\n",
      "BiLSTM Confusion Matrix:\n",
      "[[17  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  9  0  6  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0\n",
      "   0  0  0  2  0  0]\n",
      " [ 0  0  4  1  0  0  0  0  0  0 12  0  0  1  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  5  0  9  0  0  0  0  0  0  0  2  0  0  0  0  0  2  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [11  0  0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  2  0 10  0  3  0  0  0  2  0  0  0  0  0  0  0  1  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 3  0  0  0 10  0  0  0  0  1  0  0  0  0  0  0  0  0  0  4  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 17  1  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 3  0  0  0  9  0  0  0  2  1  0  0  0  0  0  0  0  0  3  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  1  0  0  5  0  0  0  0  5  0  0  0  0  0  0  0  0  0  6  1  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 18  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  2 15  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 1  0  0  4  0  1  0  0  0  0  6  1  0  0  1  0  0  3  0  0  0  0  0  0\n",
      "   0  0  0  0  0  1]\n",
      " [ 0  1  1  2  1  0  0  0  0  2  2  0  0  5  0  0  0  0  0  0  2  2  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  4  0  4  0  0  0  0  1  0  3  1  0  1  1  0  0  1  0  0  2  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [13  0  0  0  4  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  1  0  5  0  0  0  0  0  0  9  2  0  0  0  0  0  0  0  0  0  1  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0 10  0  0  0  0  0  0  2  3  0  2  0  0  0  1  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 5  0  0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0  6  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  3  0  0  0  2  1  0  0  0  0  0  0  0  0  0 12  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 2  0  0  0 13  0  0  0  1  0  0  0  0  0  0  0  0  0  1  1  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  3  0  0  0  0  0  0  2  1  0  0  0  0  0  0  0  0  0 12  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 3  0  0  0 10  0  0  0  1  1  0  0  0  0  0  0  0  0  2  0  1  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 9  0  0  0  0  0  0  0  0  0  7  0  0  0  0  0  0  0  1  0  0  0  0  1\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  5  0  1  0  0  0  0  3  0  0  2  1  0  0  1  0  0  0  0  0  1\n",
      "   4  0  0  0  0  0]\n",
      " [ 7  0  0  0  5  0  0  0  1  0  0  0  0  0  0  0  0  1  3  1  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 2  0  0  0  5  0  0  0  1  1  0  1  0  0  0  0  0  0  1  1  2  0  0  0\n",
      "   0  0  1  2  1  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0 17  0  0]\n",
      " [ 8  0  0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  5  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 6  0  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0\n",
      "   0  0  0  0  0  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Talha\\anaconda3\\envs\\my-env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\Talha\\anaconda3\\envs\\my-env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\Talha\\anaconda3\\envs\\my-env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\n",
      "ğŸ“Š GRU Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        18\n",
      "           1       1.00      1.00      1.00        18\n",
      "           2       1.00      0.78      0.88        18\n",
      "           3       1.00      1.00      1.00        18\n",
      "           4       1.00      0.78      0.88        18\n",
      "           5       0.90      1.00      0.95        18\n",
      "           6       0.94      0.94      0.94        18\n",
      "           7       0.81      0.94      0.87        18\n",
      "           8       0.65      0.94      0.77        18\n",
      "           9       0.86      1.00      0.92        18\n",
      "          10       1.00      0.89      0.94        18\n",
      "          11       0.94      0.89      0.91        18\n",
      "          12       1.00      0.89      0.94        18\n",
      "          13       0.94      0.94      0.94        18\n",
      "          14       0.94      0.94      0.94        18\n",
      "          15       0.94      0.94      0.94        18\n",
      "          16       0.95      1.00      0.97        18\n",
      "          17       0.89      0.89      0.89        18\n",
      "          18       0.88      0.83      0.86        18\n",
      "          19       0.94      0.83      0.88        18\n",
      "          20       0.88      0.83      0.86        18\n",
      "          21       1.00      1.00      1.00        18\n",
      "          22       1.00      0.89      0.94        18\n",
      "          23       0.95      1.00      0.97        18\n",
      "          24       0.85      0.94      0.89        18\n",
      "          25       0.84      0.89      0.86        18\n",
      "          26       1.00      0.89      0.94        18\n",
      "          27       1.00      1.00      1.00        18\n",
      "          28       0.89      0.94      0.92        18\n",
      "          29       0.81      0.72      0.76        18\n",
      "\n",
      "    accuracy                           0.92       540\n",
      "   macro avg       0.93      0.92      0.92       540\n",
      "weighted avg       0.93      0.92      0.92       540\n",
      "\n",
      "\n",
      "GRU Confusion Matrix:\n",
      "[[18  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0 18  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0 14  0  0  0  0  1  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0\n",
      "   2  0  0  0  0  0]\n",
      " [ 0  0  0 18  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0 14  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0 18  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 17  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 17  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 17  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 18  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  2  0  0 16  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  0  0 16  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0 16  0  0  0  0  1  0  0  0  0  0  1\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0  0  0  0 17  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0  0  0  0  0 17  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0 17  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 18  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0 16  0  0  0  0  0  0\n",
      "   1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 15  0  2  0  0  0\n",
      "   0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  1  0  0  0 15  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  1 15  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 18  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0 16  0\n",
      "   0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 18\n",
      "   0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0\n",
      "  17  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0 16  0  0  0  2]\n",
      " [ 0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0\n",
      "   0  0 16  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0 18  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0 17  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0\n",
      "   0  1  0  0  2 13]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Function to evaluate a model and print classification metrics\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š {model_name} Classification Report:\")\n",
    "    print(classification_report(y_true_classes, y_pred_classes))\n",
    "    \n",
    "    print(f\"\\n{model_name} Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true_classes, y_pred_classes))\n",
    "\n",
    "# Evaluate all four models\n",
    "evaluate_model(rnn_model, X_test_seq, y_test_encoded, \"RNN\")\n",
    "evaluate_model(lstm_model, X_test_seq, y_test_encoded, \"LSTM\")\n",
    "evaluate_model(bilstm_model, X_test_seq, y_test_encoded, \"BiLSTM\")\n",
    "evaluate_model(gru_model, X_test_seq, y_test_encoded, \"GRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1302487c-91eb-4c31-8de0-37f3d5032534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env (conda)",
   "language": "python",
   "name": "my-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
